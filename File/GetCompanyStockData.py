import requests
import bs4
import re
import os
import pandas

'''
datafame from naver has 2 table
1 is data table
2 is pagenation table
So we parsed the 0 table, means data

현재 처음 개발했던대로 requests get에 header정보를 넣어주지 않으면 안돌아간다.
또 pandas readhtml에서 바로 그냥 주식 url을 넣어줘도 안되서 일단 requests에서 받은 데이터를 2차로 넘겨줘서 동작시킴
언제또 네이버에서 막을지모름;;
'''
class GetCompanyStockData:
    __URL     = 'https://finance.naver.com/item/sise_day.nhn?code='
    __HEADER = { 'User-Agent' : ('Mozilla/5.0 (Windows NT 10.0;Win64; x64)\AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98\Safari/537.36') }
    __SAVEDIR = 'SavedStock'
    __CSV     = '.csv'
	
    def __init__(self,stockCode,stockName):
        self.stockCode = stockCode
        self.lastPageNum = 0
        self.totalStockList = []
        self.stockName = stockName
        targetFileName = os.path.join(GetCompanyStockData.__SAVEDIR,self.stockName + '_' + self.stockCode + GetCompanyStockData.__CSV)
        print(targetFileName)
        if not (os.path.exists(targetFileName)):
            self.get_last_page_number()
            self.get_stock_data_with_data_frame()
            
        else:
            print('Already exist')
            os.remove(targetFileName)
            self.get_last_page_number()
            self.get_stock_data_with_data_frame()
	
    def get_last_page_number(self):
        # request first html for getting end page
        targetUrl = GetCompanyStockData.__URL+self.stockCode + '&page=1'
        res = requests.get(targetUrl,headers = GetCompanyStockData.__HEADER)
        bs4Obj = bs4.BeautifulSoup(res.text,'html.parser')
        # find last page number attr is 'pgRR'
        pageNation = bs4Obj.find('td',class_='pgRR')
        try:
            lastPageURL = pageNation.find('a')['href']
        except:
            lastPageURL = GetCompanyStockData.__URL+self.stockCode + '&page=2'
        lastPageNumReg = re.split('page=',lastPageURL)
        lastPageNum = lastPageNumReg[1]
        self.lastPageNum = str(lastPageNum)
        self.lastPageNum = int(self.lastPageNum)
        

    def make_dir_for_save_csv(self):
        if not(os.path.isdir(GetCompanyStockData.__SAVEDIR)):
            os.makedirs(os.path.join(GetCompanyStockData.__SAVEDIR))
	
    def get_stock_data(self):
        for i in range(1,self.lastPageNum):
            res = requests.get(GetCompanyStockData.__URL + self.stockCode + '&page=' + str(i), headers = GetCompanyStockData.__HEADER)
            bs4Obj = bs4.BeautifulSoup(res.text,'html.parser')
            stockObj = bs4Obj.find_all('span',class_='tah p11')
            print(stockObj)
            self.totalStockList.append(stockObj)

    def get_stock_data_with_data_frame(self):
        self.make_dir_for_save_csv()
        stockFile = open(os.path.join(GetCompanyStockData.__SAVEDIR, self.stockName + '_' + self.stockCode + GetCompanyStockData.__CSV), mode='wt', encoding='utf-8')
        stockFile.write('date,end_price,price_change,start_price,high,low,quantity\n')
        stockFile.close()
        for i in range(0,self.lastPageNum):
            pageNum = i+1
            targetUrl = GetCompanyStockData.__URL + self.stockCode + '&page=' + str(pageNum)
            res = requests.get(targetUrl,headers = GetCompanyStockData.__HEADER)
            tempDataFrameList = pandas.read_html(res.text)
            tempDataFrame = tempDataFrameList[0] #Naver stock has 2table, one is stock data other one is pagenation table
            tempDataFrame = tempDataFrame.dropna() #drop 'na' data
            tempDataFrame.to_csv(os.path.join(GetCompanyStockData.__SAVEDIR, self.stockName + '_' + self.stockCode + GetCompanyStockData.__CSV),mode='a', header=False, index=False)
            #dataframe will be saved no index and no header, those row,col will be regenerated by code
			
if __name__ == "__main__":
    koreacenter = GetCompanyStockData('318010','팜스빌');

